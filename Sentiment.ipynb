{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estratto dal progetto di TMS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = re.compile(r\"\\n\")\n",
    "multiple_spaces = re.compile(r\"\\s+\")\n",
    "\n",
    "# pre-defined stop words\n",
    "stop_words = nltk.corpus.stopwords.words('italian') \n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(full_text, returns = returns, multiple_spaces = multiple_spaces):\n",
    "\n",
    "    text = returns.split(full_text) # split lines to interprete \"(x3)\"\n",
    "    new_text = text.lower() # make lowercase\n",
    "    new_text = new_text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) # replace punctuation with a whitespace\n",
    "    new_text = multiple_spaces.sub(\" \", new_text) # remove multiple whitespaces\n",
    "    return new_text\n",
    "\n",
    "def stopwords_remover(lyric, stop_words = stop_words):\n",
    "    new_text = []\n",
    "    for token in new_text:\n",
    "        if token not in stop_words:\n",
    "            new_text.append(token)\n",
    "    return new_text\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estratto dal progetto di DATAMAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('it_core_news_lg', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2['new_text']=df_2['original_text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "df_2['new_text_1']=0\n",
    "\n",
    "df_2=df_2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulizia da simboli\n",
    "for i in df_2.index:\n",
    "    l1=[ t for t in df_2['new_text'][i].split() if t.startswith('#') ]\n",
    "    l2=[ t for t in df_2['new_text'][i].split() if t.startswith('http') ]\n",
    "    l3=[ t for t in df_2['new_text'][i].split() if t.startswith('@') ]\n",
    "    stopword=l1+l2+l3\n",
    "    querywords=df_2['new_text'][i].split()\n",
    "    resultwords=[word for word in querywords if word.lower() not in stopword]\n",
    "    result = ' '.join(resultwords)\n",
    "    df_2['new_text_1'][i]=result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vengono cancellati tutti i termini che non sono alfanumerici o che non corrispondono a spazi vuoti\n",
    "df_2['new_text_2'] = df_2['new_text_1'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "df_2['new_text_3']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulizia da emoji e lemmatization usando la library Spacy\n",
    "for i in df_2.index:\n",
    "    stopword=['u001f600','u0001f64f','u0001f5ff','u0001f300','u0001f680','u0001f6ff','u0001f1e0','u0001f1ff','u00002702','u000027b0','u000024c2','u0001f251']\n",
    "    querywords=df_2['new_text_2'][i].split()\n",
    "    resultwords=[word for word in querywords if word.lower() not in stopword]\n",
    "    result = ' '.join(resultwords)\n",
    "    df_2['new_text_3'][i]=result\n",
    "\n",
    "stop = stopwords.words('italian')\n",
    "\n",
    "df_2['new_text_4'] = df_2['new_text_3'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "def space(comment):\n",
    "    doc = nlp(comment)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "df_2['new_text_5']= df_2['new_text_4'].apply(space)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df2666e76e52a2f771a8cfba8f6cfbfdd93be2d7973ee6ffbf7a2267d71ba071"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
