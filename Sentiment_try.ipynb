{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>author</th>\n",
       "      <th>n_rt</th>\n",
       "      <th>text_complete</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>colours_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1476028900312551426</td>\n",
       "      <td>2963478049</td>\n",
       "      <td>singerindubai</td>\n",
       "      <td>1</td>\n",
       "      <td>Ho appena visto #DontLookUp incredibile che lo...</td>\n",
       "      <td>['#NoGreenPass']</td>\n",
       "      <td>#fb1239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1475926285365006345</td>\n",
       "      <td>1364298062919794688</td>\n",
       "      <td>Lorenzo62752880</td>\n",
       "      <td>106</td>\n",
       "      <td>Anche il sindaco di Verona positivo nonostante...</td>\n",
       "      <td>['#NoGreenPass']</td>\n",
       "      <td>#fb1239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1476117524446130180</td>\n",
       "      <td>1296390431190679553</td>\n",
       "      <td>1nessuno100mil2</td>\n",
       "      <td>1</td>\n",
       "      <td>In Italia si respira una tensione enorme,  i n...</td>\n",
       "      <td>['#NoGreenPass']</td>\n",
       "      <td>#fb1239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1475960153648291841</td>\n",
       "      <td>1238800491632607232</td>\n",
       "      <td>gianluca826</td>\n",
       "      <td>60</td>\n",
       "      <td>Bassano del Grappa non si ferma. Resistenza do...</td>\n",
       "      <td>['#NoGreenPass']</td>\n",
       "      <td>#fb1239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1476095815525584896</td>\n",
       "      <td>1453282570582568960</td>\n",
       "      <td>IlDisilluso</td>\n",
       "      <td>19</td>\n",
       "      <td>\"Sono stati riportati più effetti avversi per ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>#aaaacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id            author_id           author  n_rt  \\\n",
       "0  1476028900312551426           2963478049    singerindubai     1   \n",
       "1  1475926285365006345  1364298062919794688  Lorenzo62752880   106   \n",
       "2  1476117524446130180  1296390431190679553  1nessuno100mil2     1   \n",
       "3  1475960153648291841  1238800491632607232      gianluca826    60   \n",
       "4  1476095815525584896  1453282570582568960      IlDisilluso    19   \n",
       "\n",
       "                                       text_complete          hashtags  \\\n",
       "0  Ho appena visto #DontLookUp incredibile che lo...  ['#NoGreenPass']   \n",
       "1  Anche il sindaco di Verona positivo nonostante...  ['#NoGreenPass']   \n",
       "2  In Italia si respira una tensione enorme,  i n...  ['#NoGreenPass']   \n",
       "3  Bassano del Grappa non si ferma. Resistenza do...  ['#NoGreenPass']   \n",
       "4  \"Sono stati riportati più effetti avversi per ...                []   \n",
       "\n",
       "  colours_y  \n",
       "0   #fb1239  \n",
       "1   #fb1239  \n",
       "2   #fb1239  \n",
       "3   #fb1239  \n",
       "4   #aaaacc  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "df_tweets=pd.read_csv('tweets2_nogreenpass_vaccinatevi_casapound.csv')\n",
    "df_retweeted=pd.read_csv('retweetted2_nogreenpass_vaccinatevi_casapound.csv')\n",
    "retweeted=df_retweeted.merge(df_tweets[['id_retweet','user_id_retweet','text_complete','hashtags','colours']],left_on=['id','author_id'],right_on=['id_retweet','user_id_retweet'],how='left')[['id', 'author_id', 'author','n_rt','text_complete','hashtags', 'colours_y']].reset_index(drop=True)\n",
    "retweeted=retweeted.drop_duplicates(['id','author_id','author']).reset_index(drop=True)\n",
    "retweeted.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nodes_df6=pd.read_csv('Nodes_df6.csv')\n",
    "Nodes_df6.columns\n",
    "retweetted_joined=retweeted.merge(Nodes_df6[['Id','Label','Colour']],left_on='author_id',right_on='Id',how='left')\n",
    "lista_correzione=['#nogreenpass','#nogreenpass','#vaccinatevi','#nogreenpass','#nogreenpass','#nogreenpass','#nogreenpass','#nogreenpass','#nogreenpass','#nogreenpass','#nogreenpass','#nogreenpass','#vaccinatevi']\n",
    "\n",
    "index_correzione=retweetted_joined[(retweetted_joined['Colour'].isna()) & (retweetted_joined['hashtags']=='[]')].index\n",
    "for iter,index in enumerate(index_correzione):\n",
    "    retweetted_joined.loc[index,'hashtags']=lista_correzione[iter]\n",
    "for index, row in retweetted_joined.iterrows():\n",
    "    if row['Colour'] in ['#fb1239','#99cccc']:\n",
    "        retweetted_joined.at[index,'hashtags']='#NoGreenPass'\n",
    "        retweetted_joined.at[index,'colours_y']=0\n",
    "    if row['Colour'] in ['#007474','#b00b69']:\n",
    "        retweetted_joined.at[index,'hashtags']='#vaccinatevi'\n",
    "        retweetted_joined.at[index,'colours_y']=1\n",
    "    if row['Colour'] in ['#042069']:\n",
    "        retweetted_joined.at[index,'hashtags']='#casapound'\n",
    "        retweetted_joined.at[index,'colours_y']=2\n",
    "retweeted=retweetted_joined\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric\\AppData\\Local\\Temp/ipykernel_15920/3555625496.py:21: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  retweeted['text_preprocessed'] = retweeted['text_preprocessed'].str.replace('[^\\w\\s]',' ')\n",
      "C:\\Users\\Eric\\AppData\\Local\\Temp/ipykernel_15920/3555625496.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  retweeted['text_preprocessed_new'][i]=result\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('it_core_news_lg') #, disable=['parser', 'ner'])\n",
    "#lowercase\n",
    "retweeted['text_preprocessed']=retweeted['text_complete'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#pulizia da simboli\n",
    "new_text=[]\n",
    "for index,row in retweeted.iterrows():\n",
    "    l1=[ t for t in row['text_preprocessed'].split() if t.startswith('#') ]\n",
    "    l2=[ t for t in row['text_preprocessed'].split() if t.startswith('http') ]\n",
    "    l3=[ t for t in row['text_preprocessed'].split() if t.startswith('@') ]\n",
    "    stopword=l1+l2+l3\n",
    "    querywords=row['text_preprocessed'].split()\n",
    "    resultwords=[word for word in querywords if word.lower() not in stopword]\n",
    "    result = ' '.join(resultwords)\n",
    "    new_text.append(result)\n",
    "\n",
    "retweeted['text_preprocessed']=new_text\n",
    "#vengono cancellati tutti i termini che non sono alfanumerici o che non corrispondono a spazi vuoti\n",
    "retweeted['text_preprocessed'] = retweeted['text_preprocessed'].str.replace('[^\\w\\s]',' ')\n",
    "\n",
    "stop = stopwords.words('italian')\n",
    "\n",
    "#rimozione caratteri speciali, stopwords, hyperlink, punteggiatura\n",
    "retweeted['text_preprocessed_new']=' '\n",
    "\n",
    "for i in range(0,len(retweeted.text_preprocessed)):\n",
    "    l1=[ t for t in retweeted['text_preprocessed'][i].split() if t.startswith('http') ]\n",
    "    stopword=l1+stop\n",
    "    querywords=retweeted['text_preprocessed'][i].split()\n",
    "    resultwords=[word for word in querywords if word.lower() not in stopword]\n",
    "    result = ' '.join(resultwords)\n",
    "    result=result.replace('[^\\w\\s]','')\n",
    "    result=result.translate ({ord(c): \"\" for c in \"!@#$%^&*()[]{};:,./<>?\\|`~-=_+\"})\n",
    "    retweeted['text_preprocessed_new'][i]=result\n",
    "\n",
    "#applicazione per lemmatization con spacy\n",
    "def space(comment):\n",
    "    doc = nlp(comment)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "retweeted['text_preprocessed_new']= retweeted['text_preprocessed_new'].apply(space)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparazione del lexicon\n",
    "sentix=pd.read_csv(\"sentix.csv\", header=None,delimiter=';')\n",
    "\n",
    "#riscaliamo i valori della sentiment come i precedenti valori del lexicon vader\n",
    "#consideriamo valori estremi di vader lexicon -4 e +4\n",
    "#consideriamo valori estremi di sentix lexicon -1 e +1\n",
    "\n",
    "with open(\"sentix.txt\",'w',encoding=\"utf-8\") as txt_file:\n",
    "    for index,line in sentix.iterrows():\n",
    "        scaled_score=(((float(line[5]) + 1)/2)*(8))-4\n",
    "        txt_file.write(str(line[0])+'\\t'+str(scaled_score)+'\\n')\n",
    "\n",
    "#creo un dizionario per updatare il lexicon nltk di vader\n",
    "d= dict([(parola,score) for parola, score in zip(sentix[0],sentix[5])])\n",
    "sentix_dict=d\n",
    "\n",
    "\n",
    "#arricchimento vader lexicon per multi-vader\n",
    "with open('lexicon_vader_enriched.txt','w',encoding=\"utf-8\") as newfile:\n",
    "    with open('vader_lexicon.txt') as vader_file:\n",
    "        for line in vader_file:\n",
    "            newfile.write(line)\n",
    "    with open('sentix.txt',encoding=\"utf-8\") as sentix_file:\n",
    "        for line in sentix_file:\n",
    "            newfile.write(line)\n",
    "#Algoritmo nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentix_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15920/1270839655.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mAnalyzer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mAnalyzer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexicon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentix_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m#vader-multi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mvaderSentiment\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvaderSentiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentix_dict' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import vader\n",
    "\n",
    "Analyzer = vader.SentimentIntensityAnalyzer()\n",
    "\n",
    "Analyzer.lexicon.update(sentix_dict)\n",
    "#vader-multi\n",
    "from vaderSentiment import vaderSentiment\n",
    "\n",
    "#non facciamo pulizia del testo sulle emojii\n",
    "analyzer = vaderSentiment.SentimentIntensityAnalyzer(lexicon_file='lexicon_vader_enriched.txt',emoji_lexicon='emoji_utf8_lexicon.txt')\n",
    "\n",
    "#il termine \"positivo\" è stato modificato associandolo a un sentiment negativo\n",
    "#il termine \"negativo\" è associato a un sentiment neutro\n",
    "\n",
    "#vader\n",
    "from vader_sentiment.vader_sentiment import SentimentIntensityAnalyzer\n",
    "analyzer2=SentimentIntensityAnalyzer(lexicon_file='sentix.txt',emoji_lexicon='emoji_utf8_lexicon.txt')\n",
    "#varie prove di confronto tra i 3 algoritmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment import vaderSentiment\n",
    "\n",
    "#non facciamo pulizia del testo sulle emojii\n",
    "analyzer = vaderSentiment.SentimentIntensityAnalyzer(lexicon_file='lexicon_vader_enriched.txt',emoji_lexicon='emoji_utf8_lexicon.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testo=retweeted['text_preprocessed'][16]\n",
    "#testo='positivo'\n",
    "\n",
    "print(testo)\n",
    "print('\\n')\n",
    "print('NLTK: ',Analyzer.polarity_scores(testo))\n",
    "print('VADER-MULTI: ', analyzer.polarity_scores(testo)) #algoritmo migliore\n",
    "print('VADER: ', analyzer2.polarity_scores(testo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list=list(retweeted[retweeted.sentiment==0.0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception handled, iteration reached:  428\n",
      "exception:  HTTP Error 429: Too Many Requests\n"
     ]
    }
   ],
   "source": [
    "#retweeted=retweeted.reset_index(drop=True)\n",
    "#retweeted['sentiment']=0.0\n",
    "\n",
    "for index in index_list:\n",
    "    if index > 337 :\n",
    "        try:\n",
    "            retweeted.at[index,'sentiment']=analyzer.polarity_scores(retweeted.loc[index,'text_preprocessed'])['compound']\n",
    "        except Exception as e:\n",
    "            print('exception handled, iteration reached: ',index)\n",
    "            print('exception: ',e)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweeted.to_csv('retweeted_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_vax=retweeted[(retweeted.colours_y==0) | (retweeted.colours_y==2)]['text_preprocessed_new']\n",
    "pro_vax=retweeted[(retweeted.colours_y==1)]['text_preprocessed_new']\n",
    "\n",
    "#creazione lista novax provax\n",
    "listone_novax=' '.join(no_vax)\n",
    "listone_provax=' '.join(pro_vax)\n",
    "\n",
    "listone_novax= deEmojify(listone_novax)\n",
    "listone_provax= deEmojify(listone_provax)\n",
    "\n",
    "#creazione wordcloud con calibrazione dei parametri\n",
    "wordcloud_novax = WordCloud(width = 2000, height = 1000, random_state=1, prefer_horizontal=1,\n",
    "                              background_color='white', colormap='Set2', \n",
    "                              collocations=False,max_words=75,relative_scaling=0.5,\n",
    "                              stopwords=['potere','essere','dovere','ogni','tre','poi','cè','me','qui','lamore','volere','fare','cosa','stare','l','e','mai','così','tutto','ciò','avere','molto','anno','tanto','quindi','grazia','dire','solo','andare','giorno','te','prendere','vaccinare','nn','gg','dm','dose','quando','già']).generate(listone_novax)\n",
    "\n",
    "wordcloud_provax = WordCloud(width = 2000, height = 1000, random_state=1, prefer_horizontal=1,\n",
    "                               background_color='white', colormap='Set2',\n",
    "                               collocations=False,max_words=75,relative_scaling=.5,\n",
    "                               stopwords=['potere','essere','dovere','ogni','tre','poi','cè','me','qui','lamore','volere','fare','cosa','stare','l','e','mai','così','tutto','ciò','avere','molto','anno','tanto','quindi','grazia','dire','solo','andare','giorno','te','prendere','vaccinare','nn','gg','dm','dose','quando','già']).generate(listone_provax)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_cloud(wordcloud):\n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    # Display image\n",
    "    plt.imshow(wordcloud) \n",
    "    # No axis details\n",
    "    plt.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df2666e76e52a2f771a8cfba8f6cfbfdd93be2d7973ee6ffbf7a2267d71ba071"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
